# LLM Configuration
# For Linux with host networking: use localhost
# For macOS/Windows with bridge networking: use host.docker.internal
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_MODEL=ollama/llama3.2

# API Keys (Optional)
# ANTHROPIC_API_KEY=your_key_here
# OPENAI_API_KEY=your_key_here

# App Settings
LOG_LEVEL=INFO
DEBUG=False
